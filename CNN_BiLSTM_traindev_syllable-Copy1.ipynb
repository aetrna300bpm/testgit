{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f3fdeff-6621-43b0-b165-0a12cfa247c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\GIGABYTE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\GIGABYTE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, Conv1D, LSTM, Dense, Bidirectional\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import json\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1295f0a9-ec20-47e5-9de6-5baf49ef091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON\n",
    "\n",
    "def load_data_from_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "\n",
    "    words = [entry[\"words\"] for entry in data]\n",
    "    tags = [entry[\"tags\"] for entry in data]\n",
    "\n",
    "    return words, tags\n",
    "\n",
    "\n",
    "train_words, train_tags = load_data_from_json('PhoNER_COVID19-main-BIO/PhoNER_COVID19-main/data/syllable/train_syllable.json')\n",
    "dev_words, dev_tags = load_data_from_json('PhoNER_COVID19-main-BIO/PhoNER_COVID19-main/data/syllable/dev_syllable.json')\n",
    "test_words, test_tags = load_data_from_json('PhoNER_COVID19-main-BIO/PhoNER_COVID19-main/data/syllable/test_syllable.json')\n",
    "\n",
    "def merge_data(words1, tags1, words2, tags2):\n",
    "    merged_words = words1 + words2\n",
    "    merged_tags = tags1 + tags2\n",
    "    return merged_words, merged_tags\n",
    "\n",
    "traindev_words, traindev_tags = merge_data(train_words, train_tags, dev_words, dev_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ef44e79-4666-4bd0-88bd-3dd80866e826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_words)\n",
    "\n",
    "# Đổi chữ thành ID\n",
    "traindev_sequences = tokenizer.texts_to_sequences(traindev_words)\n",
    "train_sequences = tokenizer.texts_to_sequences(train_words)\n",
    "dev_sequences = tokenizer.texts_to_sequences(dev_words)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_words)\n",
    "\n",
    "\n",
    "# Đổi tag thành ID\n",
    "tag_to_index = {'O': 0, 'B-AGE': 1, 'B-DATE': 2, 'B-GENDER': 3, 'B-JOB': 4, 'B-LOCATION': 5, 'B-NAME': 6, 'B-ORGANIZATION': 7, 'B-PATIENT_ID': 8, 'B-SYMPTOM_AND_DISEASE': 9, 'B-TRANSPORTATION': 10, 'I-AGE': 11, 'I-DATE': 12, 'I-GENDER': 13, 'I-JOB': 14, 'I-LOCATION': 15, 'I-NAME': 16, 'I-ORGANIZATION': 17, 'I-PATIENT_ID': 18, 'I-SYMPTOM_AND_DISEASE': 19, 'I-TRANSPORTATION': 20}\n",
    "\n",
    "traindev_tags = [[tag_to_index.get(tag) for tag in seq] for seq in traindev_tags]\n",
    "train_tags = [[tag_to_index.get(tag) for tag in seq] for seq in train_tags]\n",
    "dev_tags = [[tag_to_index.get(tag) for tag in seq] for seq in dev_tags]\n",
    "test_tags = [[tag_to_index.get(tag) for tag in seq] for seq in test_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "064db00c-8a62-4eb5-9bbd-841c60c6de38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ID of '\"' is: 9\n"
     ]
    }
   ],
   "source": [
    "# Get the word-to-index mapping\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Print the ID of a specific token\n",
    "token_to_check = '\"'\n",
    "token_id = word_index.get(token_to_check)\n",
    "\n",
    "if token_id is not None:\n",
    "    print(f\"The ID of '{token_to_check}' is: {token_id}\")\n",
    "else:\n",
    "    print(f\"{token_to_check} is not in the vocabulary.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3eceb815-be16-4d5e-ac7c-95f01f389026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tokens(data, tags, tokens_to_remove):\n",
    "    new_data = []\n",
    "    new_tags = []\n",
    "    for seq, tag_seq in zip(data, tags):\n",
    "        new_seq = [word for word in seq if word not in tokens_to_remove]\n",
    "        new_tag_seq = [tag for word, tag in zip(seq, tag_seq) if word not in tokens_to_remove]\n",
    "        new_data.append(new_seq)\n",
    "        new_tags.append(new_tag_seq)\n",
    "    return new_data, new_tags\n",
    "\n",
    "tokens_to_remove = [1, 4, 9, 32, 33, 151, 769]\n",
    "\n",
    "# Remove instances of the specified tokens\n",
    "traindev_sequences, traindev_tags = remove_tokens(traindev_sequences, traindev_tags, tokens_to_remove)\n",
    "train_sequences, train_tags = remove_tokens(train_sequences, train_tags, tokens_to_remove)\n",
    "dev_sequences, dev_tags = remove_tokens(dev_sequences, dev_tags, tokens_to_remove)\n",
    "test_sequences, test_tags = remove_tokens(test_sequences, test_tags, tokens_to_remove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8658a46c-15de-46ab-9f47-6bca742d0a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56, 2, 7, 11, 210, 31, 64, 2, 3, 140, 99, 299, 8, 190, 105, 81, 661, 870, 142, 512, 11, 347, 147, 188, 109, 2, 3, 63, 770, 95, 47, 12, 68, 228, 49, 172, 157, 745, 247, 871, 109, 2, 3, 143, 2, 905, 2429, 223, 61, 11, 88, 237, 12, 24, 35]\n"
     ]
    }
   ],
   "source": [
    "# Print the modified data\n",
    "print(train_sequences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "679f0ed3-86e4-4589-95b0-3386169de32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad chuỗi entry theo câu\n",
    "max_seq_len = max_seq_len = 150\n",
    "traindev_data = pad_sequences(traindev_sequences, maxlen=max_seq_len, padding='post', truncating='post')\n",
    "train_data = pad_sequences(dev_sequences, maxlen=max_seq_len, padding='post', truncating='post')\n",
    "dev_data = pad_sequences(dev_sequences, maxlen=max_seq_len, padding='post', truncating='post')\n",
    "test_data = pad_sequences(test_sequences, maxlen=max_seq_len, padding='post', truncating='post')\n",
    "\n",
    "# Pad chuỗi tag theo câu\n",
    "traindev_tags = pad_sequences(traindev_tags, maxlen=max_seq_len, padding='post')\n",
    "train_tags = pad_sequences(train_tags, maxlen=max_seq_len, padding='post')\n",
    "dev_tags = pad_sequences(dev_tags, maxlen=max_seq_len, padding='post')\n",
    "test_tags = pad_sequences(test_tags, maxlen=max_seq_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eb6801f-017f-4a76-9d9a-3fc02c496bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2    3  808   37 1233   34   15    3  117  518 2154 1665 1485  171\n",
      "  821  411 1923   58   55  209   10   92  348   21  680  150   21    2\n",
      "    3 1028  134   15    3  117 1665 2154   54  191  222   38   39    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fea4dfac-8555-41ff-b1c3-fa2a960117cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module load file w2v\n",
    "def load_embeddings(file_path):\n",
    "    embeddings = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            word = parts[0]\n",
    "            vector = list(map(float, parts[-300:])) #  dims vector embedding\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "def create_embedding_matrix(tokenizer, embeddings, embedding_dim):\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "embedding_file_path = 'word2vec_vi_syllables_300dims.txt' # Input path file embedding (.txt)\n",
    "word_embeddings = load_embeddings(embedding_file_path)\n",
    "embedding_dim = 300  # dims vector embedding\n",
    "\n",
    "\n",
    "# Load pre-train embedding\n",
    "embeddings = load_embeddings(embedding_file_path)\n",
    "# Embedding matrix\n",
    "embedding_matrix = create_embedding_matrix(tokenizer, embeddings, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53184d15-d40e-4e97-9564-f7fb72dc8d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  98    2    3   20  125  171 1005  422   11   31   57  111   16    2\n",
      "    3   86  325  194    2    3  852   67   36   34  798   37 1068   34\n",
      "  207  724   37  375   34  713   67  666   34   15 3283  182   48    2\n",
      "    3   86  325    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(dev_data[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d806b04-87b1-40d3-b515-6b2ccb58764a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  5 15 15 15  0  0  0  0  0  0  0  0  8  0  0  0  8  3  1  0\n",
      "  8  3  1  0  0  8  3  1  0  8  3  1  0  0  0  0  0  0  0  0  8  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "print(dev_tags[42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adc53000-2a72-4c45-a5c3-232b5153485e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 150, 300)          1019400   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 150, 64)           57664     \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 150, 128)          66048     \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               (None, 150, 21)           2709      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1145821 (4.37 MB)\n",
      "Trainable params: 1145821 (4.37 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_units = 64\n",
    "num_classes = len(tag_to_index)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_seq_len, trainable=True)) # trainable = False\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu',padding='same'))\n",
    "model.add(Bidirectional(LSTM(units=lstm_units, return_sequences=True)))  # Bidirectional LSTM\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "870bee3a-1f2f-45d5-a8bc-2d9f473705e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "220/220 [==============================] - 20s 78ms/step - loss: 0.3085 - accuracy: 0.9558 - val_loss: 0.1204 - val_accuracy: 0.9684\n",
      "Epoch 2/10\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 0.0567 - accuracy: 0.9851 - val_loss: 0.0852 - val_accuracy: 0.9784\n",
      "Epoch 3/10\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 0.0362 - accuracy: 0.9905 - val_loss: 0.0789 - val_accuracy: 0.9802\n",
      "Epoch 4/10\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 0.0294 - accuracy: 0.9923 - val_loss: 0.0756 - val_accuracy: 0.9808\n",
      "Epoch 5/10\n",
      "220/220 [==============================] - 17s 75ms/step - loss: 0.0257 - accuracy: 0.9933 - val_loss: 0.0752 - val_accuracy: 0.9813\n",
      "Epoch 6/10\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 0.0229 - accuracy: 0.9939 - val_loss: 0.0761 - val_accuracy: 0.9814\n",
      "Epoch 7/10\n",
      "220/220 [==============================] - 16s 74ms/step - loss: 0.0211 - accuracy: 0.9944 - val_loss: 0.0765 - val_accuracy: 0.9812\n",
      "Epoch 8/10\n",
      "220/220 [==============================] - 16s 75ms/step - loss: 0.0194 - accuracy: 0.9948 - val_loss: 0.0745 - val_accuracy: 0.9818\n",
      "Epoch 9/10\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 0.0181 - accuracy: 0.9952 - val_loss: 0.0744 - val_accuracy: 0.9816\n",
      "Epoch 10/10\n",
      "220/220 [==============================] - 17s 76ms/step - loss: 0.0170 - accuracy: 0.9955 - val_loss: 0.0757 - val_accuracy: 0.9820\n",
      "94/94 [==============================] - 2s 24ms/step - loss: 0.0757 - accuracy: 0.9820\n",
      "Accuracy: 98.20%\n"
     ]
    }
   ],
   "source": [
    "model.fit(traindev_data, traindev_tags, validation_data=(test_data, test_tags), epochs=10, batch_size=32)\n",
    "\n",
    "\n",
    "_, accuracy = model.evaluate(test_data, test_tags)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be930093-926d-4cb4-948a-3b70ed52b568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 3s 21ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "                    O       0.99      0.99      0.99    421675\n",
      "                B-AGE       0.73      0.67      0.70       517\n",
      "               B-DATE       0.87      0.89      0.88      1577\n",
      "             B-GENDER       0.79      0.70      0.75       396\n",
      "                B-JOB       0.71      0.37      0.49       171\n",
      "           B-LOCATION       0.71      0.71      0.71      4260\n",
      "               B-NAME       0.79      0.62      0.69       252\n",
      "       B-ORGANIZATION       0.85      0.74      0.79       768\n",
      "         B-PATIENT_ID       0.88      0.88      0.88      1657\n",
      "B-SYMPTOM_AND_DISEASE       0.80      0.77      0.78      1109\n",
      "     B-TRANSPORTATION       0.75      0.60      0.67       157\n",
      "                I-AGE       0.00      0.00      0.00         6\n",
      "               I-DATE       0.90      0.95      0.92      1665\n",
      "             I-GENDER       0.00      0.00      0.00         1\n",
      "                I-JOB       0.68      0.37      0.47       334\n",
      "           I-LOCATION       0.82      0.87      0.85      9702\n",
      "               I-NAME       0.83      0.41      0.55        71\n",
      "       I-ORGANIZATION       0.89      0.81      0.84      3586\n",
      "         I-PATIENT_ID       0.00      0.00      0.00        20\n",
      "I-SYMPTOM_AND_DISEASE       0.81      0.81      0.81      2021\n",
      "     I-TRANSPORTATION       0.96      0.49      0.65        55\n",
      "\n",
      "             accuracy                           0.98    450000\n",
      "            macro avg       0.70      0.60      0.64    450000\n",
      "         weighted avg       0.98      0.98      0.98    450000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_data)\n",
    "pred_tags = np.argmax(predictions, axis=-1)\n",
    "\n",
    "print(classification_report(test_tags.flatten(), pred_tags.flatten(), target_names=list(tag_to_index.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b9fd61-5aaa-4563-98f6-cf8d61730be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JOB luôn là tag perform tệ nhất ở các lần thử, các model khác nhau, do ít dữ liệu train?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5fee71-bb0f-40d6-9cd2-b845e645b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_data)\n",
    "pred_tags = np.argmax(predictions, axis=-1)\n",
    "\n",
    "print(classification_report(test_tags.flatten(), pred_tags.flatten(), target_names=list(tag_to_index.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb64442c-5ca8-4aee-8188-7df5c2c0e886",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
